{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:40:49.755562Z","iopub.execute_input":"2025-05-18T03:40:49.755873Z","iopub.status.idle":"2025-05-18T03:40:49.762336Z","shell.execute_reply.started":"2025-05-18T03:40:49.755851Z","shell.execute_reply":"2025-05-18T03:40:49.761424Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}],"execution_count":199},{"cell_type":"markdown","source":"# Introduction\nIn this notebook, I’ll be coding my own neural network from scratch for the MNIST dataset. Each data point is a 28×28 image (784 pixels), so our input layer has 784 neurons. Since this is a 10-class classification task (digits 0–9), our output layer will consist of 10 neurons and use a Softmax activation. I’ll include two hidden layers with 10 neurons  using ReLU activation—just because I get good vibes from it.\n\n```python\n# Layer definitions\nINPUT_SIZE   = 28 * 28    # 784 input neurons (pixels)\nHIDDEN_SIZE  = 17         # 17 hidden neurons (ReLU activation)\nOUTPUT_SIZE  = 10         # 10 output neurons (Softmax for digits 0–9)\n\n# Activation functions\nACTIVATION_HIDDEN = 'ReLU'\nACTIVATION_OUTPUT = 'Softmax'\n","metadata":{}},{"cell_type":"markdown","source":"# The Data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain = pd.read_csv(\"/kaggle/input/digit-recognizer/train.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:40:57.071006Z","iopub.execute_input":"2025-05-18T03:40:57.071370Z","iopub.status.idle":"2025-05-18T03:40:59.188985Z","shell.execute_reply.started":"2025-05-18T03:40:57.071348Z","shell.execute_reply":"2025-05-18T03:40:59.188212Z"}},"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":200},{"cell_type":"code","source":"train = np.array(train)\nm , n = train.shape\nn = n - 1\nnp.random.shuffle(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:41:00.545865Z","iopub.execute_input":"2025-05-18T03:41:00.546158Z","iopub.status.idle":"2025-05-18T03:41:01.245496Z","shell.execute_reply.started":"2025-05-18T03:41:00.546136Z","shell.execute_reply":"2025-05-18T03:41:01.244353Z"}},"outputs":[],"execution_count":201},{"cell_type":"code","source":"X_train = train[:, 1:].T\nY_train = train[:, 0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:41:02.310442Z","iopub.execute_input":"2025-05-18T03:41:02.311181Z","iopub.status.idle":"2025-05-18T03:41:02.315185Z","shell.execute_reply.started":"2025-05-18T03:41:02.311153Z","shell.execute_reply":"2025-05-18T03:41:02.314257Z"}},"outputs":[],"execution_count":202},{"cell_type":"markdown","source":"Since these are pixel values form 0 to 255, we normalize our data by scaling by 255","metadata":{}},{"cell_type":"code","source":"X_train = X_train / 255.0\n\nX_train[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:41:03.372812Z","iopub.execute_input":"2025-05-18T03:41:03.373132Z","iopub.status.idle":"2025-05-18T03:41:03.565498Z","shell.execute_reply.started":"2025-05-18T03:41:03.373106Z","shell.execute_reply":"2025-05-18T03:41:03.564765Z"}},"outputs":[{"execution_count":203,"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{}}],"execution_count":203},{"cell_type":"markdown","source":"# Initializing weights and biases.\n\nNow, we must initialize our weights and biases. Since we use the ReLU activation layer, I decided to go with He Initialization\n\nHe Initialization is a method of weight initialization using $n_i$ and $n_o$, our input and output neurons, where $n_i = 784$ and $n_o$ = 1. \n\nThe He Intiialization method initializes the weights and biases as a normal distribution function with $\\mu = 0$ and $\\sigma = \\sqrt{\\frac{2}{n_1}}$, so in our case, $\\sigma = \\sqrt{\\frac{2}{784}}$. ","metadata":{}},{"cell_type":"code","source":"import math\ndef initialize_values():\n    W1 = np.random.randn(10, 784) * np.sqrt(2 / 784)  # He initialization for layer 1\n    B1 = np.zeros((10, 1))                            # Biases typically initialized to 0\n    W2 = np.random.randn(10, 10) * np.sqrt(2 / 10)    # He initialization for layer 2\n    B2 = np.zeros((10, 1))\n    return W1, B1, W2, B2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:42:33.504962Z","iopub.execute_input":"2025-05-18T03:42:33.505278Z","iopub.status.idle":"2025-05-18T03:42:33.510816Z","shell.execute_reply.started":"2025-05-18T03:42:33.505254Z","shell.execute_reply":"2025-05-18T03:42:33.509866Z"}},"outputs":[],"execution_count":204},{"cell_type":"markdown","source":"## Plotted Example","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\n# define the number of inputs from 1 to 100\nvalues = [i for i in range(1, 101)]\n# calculate the range for each number of inputs\nresults = [math.sqrt(2.0 / n) for n in values]\n# create an error bar plot centered on 0 for each number of inputs\npyplot.errorbar(values, [0.0 for _ in values], yerr=results)\npyplot.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:42:35.299357Z","iopub.execute_input":"2025-05-18T03:42:35.299705Z","iopub.status.idle":"2025-05-18T03:42:35.446499Z","shell.execute_reply.started":"2025-05-18T03:42:35.299673Z","shell.execute_reply":"2025-05-18T03:42:35.445515Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":205},{"cell_type":"markdown","source":"# One hot encoder\n\nConverts a vector into its one-hot encoded form","metadata":{}},{"cell_type":"code","source":"def one_hot(Y):\n    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n    one_hot_Y[np.arange(Y.size), Y] = 1\n    return one_hot_Y.T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:43:14.202044Z","iopub.execute_input":"2025-05-18T03:43:14.202344Z","iopub.status.idle":"2025-05-18T03:43:14.206994Z","shell.execute_reply.started":"2025-05-18T03:43:14.202321Z","shell.execute_reply":"2025-05-18T03:43:14.206044Z"}},"outputs":[],"execution_count":206},{"cell_type":"markdown","source":"# ReLU\n\nReLU : Rectified Linear Unit is a popular activation function. Activation funcitons are added to our neural network to introduce non-linearlity. ReLU is a non-linear transform such that\n\n$ReLU(x) = max(0,x)$\n","metadata":{}},{"cell_type":"code","source":"def relu(x):\n    return np.maximum(0,x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:43:16.370507Z","iopub.execute_input":"2025-05-18T03:43:16.371352Z","iopub.status.idle":"2025-05-18T03:43:16.375768Z","shell.execute_reply.started":"2025-05-18T03:43:16.371310Z","shell.execute_reply":"2025-05-18T03:43:16.374629Z"}},"outputs":[],"execution_count":207},{"cell_type":"markdown","source":"# Softmax\n\nSoftmax is the most popular activation function for multi-class classification problems. It transforms the raw outputs (logits) from the final layer into a probability distribution, where each value lies in [0, 1] and all values sum to 1.\n\nLet **p** = (p₁, p₂, …, pₙ) be the vector of logits. Then the Softmax function σ applied to component pᵢ is given by:\n\n$$\n\\sigma(p_i) \\;=\\; \\frac{e^{p_i}}{\\sum_{j=1}^{n} e^{p_j}}\n$$","metadata":{}},{"cell_type":"code","source":"def softmax(Z):\n    return np.exp(Z) / sum(np.exp(Z))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:43:50.544503Z","iopub.execute_input":"2025-05-18T03:43:50.544832Z","iopub.status.idle":"2025-05-18T03:43:50.549945Z","shell.execute_reply.started":"2025-05-18T03:43:50.544810Z","shell.execute_reply":"2025-05-18T03:43:50.548857Z"}},"outputs":[],"execution_count":208},{"cell_type":"markdown","source":"# Forward Propogation","metadata":{}},{"cell_type":"markdown","source":"A forward pass proceeds as follows:\n\n1. **Input**  \n   Let  \n   $$\n   X \\in \\mathbb{R}^{784 \\times 33600}\n   $$  \n   be the matrix of flattened images (784 features, 33 600 examples).\n\n2. **First layer linear step**  \n   $$\n   Z_1 = W_1\\,X + b_1\n   \\quad\n   (W_1\\in\\mathbb{R}^{17\\times 784},\\;b_1\\in\\mathbb{R}^{17\\times1})\n   $$  \n   so that  \n   $$\n   Z_1 \\in \\mathbb{R}^{17\\times 33600}.\n   $$\n\n3. **First layer activation (ReLU)**  \n   $$\n   A_1 = \\mathrm{ReLU}(Z_1)\n   \\quad\\text{with}\\quad\n   A_1 \\in \\mathbb{R}^{17\\times 33600}.\n   $$\n\n4. **Second layer linear step**  \n   $$\n   Z_2 = W_2\\,A_1 + b_2\n   \\quad\n   (W_2\\in\\mathbb{R}^{10\\times 17},\\;b_2\\in\\mathbb{R}^{10\\times1})\n   $$  \n   yielding  \n   $$\n   Z_2 \\in \\mathbb{R}^{10\\times 33600}.\n   $$\n\n5. **Output activation (Softmax)**  \n   $$\n   A_{2} = \\mathrm{Softmax}(Z_2)\n   \\quad\\text{with}\\quad\n   A_{2} \\in \\mathbb{R}^{10\\times 33600},\n   $$  \n   which gives the predicted probability for each of the 10 classes per example.  \n","metadata":{}},{"cell_type":"code","source":"def forward_propagation(W1, B1, W2, B2, X):\n  Z1 = W1.dot(X) + B1\n  A1 = relu(Z1)\n  Z2 = W2.dot(A1) + B2\n  A2 = softmax(Z2)\n  return Z1, A1, Z2, A2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:44:20.613216Z","iopub.execute_input":"2025-05-18T03:44:20.613595Z","iopub.status.idle":"2025-05-18T03:44:20.619393Z","shell.execute_reply.started":"2025-05-18T03:44:20.613551Z","shell.execute_reply":"2025-05-18T03:44:20.618451Z"}},"outputs":[],"execution_count":209},{"cell_type":"markdown","source":"# Backward Propogation\n\nA backward pass computes the gradients of the loss with respect to each parameter by applying the chain rule layer by layer. Let $$m$$ be the number of examples. First, we compute the error at the output layer:  \n$$\ndZ^{[2]} = A^{[2]} - Y_{\\text{cap}}\n$$  \nThen we get the gradients for the second layer parameters:  \n$$\ndW^{[2]} = \\frac{1}{m}\\,dZ^{[2]}\\,(A^{[1]})^T,\n\\quad\ndB^{[2]} = \\frac{1}{m}\\sum dZ^{[2]}\n$$  \nNext, we propagate the error through the ReLU nonlinearity in layer 1:  \n$$\ndZ^{[1]} = (W^{[2]})^T\\,dZ^{[2]}\\;\\ast\\;\\mathbf{1}(Z^{[1]}>0)\n$$  \nFinally, we compute the gradients for the first layer:  \n$$\ndW^{[1]} = \\frac{1}{m}\\,dZ^{[1]}\\,X^T,\n\\quad\ndB^{[1]} = \\frac{1}{m}\\sum dZ^{[1]}\n$$\n\n","metadata":{}},{"cell_type":"code","source":"def backward_propagation(W1, B1, W2, B2, Z1, A1, Z2, A2, X, Y):\n  one_hot_Y = one_hot_converter(Y)\n  dZ2 = A2 - one_hot_Y\n  dW2 = 1 / m * dZ2.dot(A1.T)\n  dB2 = 1 / m * np.sum(dZ2)\n  dZ1 = W2.T.dot(dZ2) * (Z1 > 0)\n  dW1 = 1 / m * dZ1.dot(X.T)\n  dB1 = 1 / m * np.sum(dZ1)\n  return dW1, dB1, dW2, dB2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:39:01.206639Z","iopub.execute_input":"2025-05-18T03:39:01.206996Z","iopub.status.idle":"2025-05-18T03:39:01.213806Z","shell.execute_reply.started":"2025-05-18T03:39:01.206969Z","shell.execute_reply":"2025-05-18T03:39:01.212837Z"}},"outputs":[],"execution_count":195},{"cell_type":"markdown","source":"# Update Pass","metadata":{}},{"cell_type":"markdown","source":"In the update pass, we apply gradient descent to adjust each parameter by its gradient scaled by the learning rate $$\\eta$$ Concretely, for each layer $$l$$\n\n$$\nW^{[l]} \\;\\;=\\;\\; W^{[l]} \\;-\\; \\eta\\,dW^{[l]}, \n\\quad\nb^{[l]} \\;\\;=\\;\\; b^{[l]} \\;-\\; \\eta\\,dB^{[l]}.\n$$\n\nFor our two-layer network this becomes:\n\n$$\nW_1 \\;=\\; W_1 - \\text{lr}\\,\\bigl(dW_1\\bigr),\n\\quad\nb_1 \\;=\\; b_1 - \\text{lr}\\,\\bigl(dB_1\\bigr),\n$$\n\n$$\nW_2 \\;=\\; W_2 - \\text{lr}\\,\\bigl(dW_2\\bigr),\n\\quad\nb_2 \\;=\\; b_2 - \\text{lr}\\,\\bigl(dB_2\\bigr).\n$$\n\n","metadata":{}},{"cell_type":"code","source":"def update_parameters(W1, B1, W2, B2, dW1, dB1, dW2, dB2, learning_rate):\n  W1 = W1 - learning_rate * dW1\n  B1 = B1 - learning_rate * dB1\n  W2 = W2 - learning_rate * dW2\n  B2 = B2 - learning_rate * dB2\n  return W1, B1, W2, B2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:44:56.769354Z","iopub.execute_input":"2025-05-18T03:44:56.769669Z","iopub.status.idle":"2025-05-18T03:44:56.775189Z","shell.execute_reply.started":"2025-05-18T03:44:56.769647Z","shell.execute_reply":"2025-05-18T03:44:56.774323Z"}},"outputs":[],"execution_count":210},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"At prediction time, we take the output probabilities from the Softmax layer, $(A^{[2]}\\in\\mathbb{R}^{10\\times m})$, and choose the class with the highest probability for each example. Mathematically, for the $i^{th}$ example:\n\n$$\n\\hat{y}^{(i)} = \\arg\\max_{k}\\;A^{[2]}_{k,i}.\n$$","metadata":{}},{"cell_type":"code","source":"def predict(A2):\n    return np.argmax(A2,0) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:37:11.278428Z","iopub.execute_input":"2025-05-18T03:37:11.279040Z","iopub.status.idle":"2025-05-18T03:37:11.283760Z","shell.execute_reply.started":"2025-05-18T03:37:11.279014Z","shell.execute_reply":"2025-05-18T03:37:11.282759Z"}},"outputs":[],"execution_count":189},{"cell_type":"markdown","source":"# Accuracy","metadata":{}},{"cell_type":"markdown","source":"Accuracy measures the fraction of correct predictions over all examples. Let $m$ be the number of examples, $\\hat y^{(i)}$ the predicted label and $y^{(i)}$ the true label. Then\n\n$$\n\\text{Accuracy} \\;=\\; \\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{1}\\bigl(\\hat y^{(i)} = y^{(i)}\\bigr)\n$$","metadata":{}},{"cell_type":"code","source":"def accuracy(A2, Y):\n    return np.sum(A2 == Y)/Y.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:37:12.480985Z","iopub.execute_input":"2025-05-18T03:37:12.481337Z","iopub.status.idle":"2025-05-18T03:37:12.486512Z","shell.execute_reply.started":"2025-05-18T03:37:12.481314Z","shell.execute_reply":"2025-05-18T03:37:12.485436Z"}},"outputs":[],"execution_count":190},{"cell_type":"markdown","source":"# Gradient Descent","metadata":{}},{"cell_type":"markdown","source":"The function $$\\mathrm{grad}(X, Y, \\alpha, n)$$ implements gradient descent for a two-layer neural network, where $$\\alpha$$ is the learning rate and $$n$$ is the number of epochs. It initializes the parameters $$W_1, B_1, W_2, B_2$$, then for each epoch $$i=0,\\dots,n-1$$\n\n1. **Forward pass** to compute  \n   $$\n   Z_1, A_1, Z_2, A_2\n   $$\n\n2. **Backward pass** to obtain gradients  \n   $$\n   dW_1, dB_1, dW_2, dB_2\n   $$\n\n3. **Update step** via  \n   $$\n   W_\\ell \\;\\leftarrow\\; W_\\ell \\;-\\;\\alpha\\,dW_\\ell,\n   \\quad\n   B_\\ell \\;\\leftarrow\\; B_\\ell \\;-\\;\\alpha\\,dB_\\ell\n   $$\n\nEvery 10 epochs it computes predictions  \n$$\n\\hat Y = \\arg\\max A_2\n$$  \nand prints the accuracy  \n$$\n\\frac{1}{m}\\sum_{i=1}^{m}\\mathbf{1}\\bigl(\\hat y^{(i)} = y^{(i)}\\bigr).\n$$  \n\nAfter completing $$n$$ epochs, it returns the optimized parameters $$W_1, B_1, W_2, B_2$$.  \n","metadata":{}},{"cell_type":"code","source":"def gradient_descent(X, Y, alpha, iterations):\n  W1, B1, W2, B2 = initialize_parameters()\n\n  for i in range(iterations):\n    Z1, A1, Z2, A2 = forward_propagation(W1, B1, W2, B2, X)\n    dW1, dB1, dW2, dB2 = backward_propagation(W1, B1, W2, B2, Z1, A1, Z2, A2, X, Y)\n    W1, B1, W2, B2 = update_parameters(W1, B1, W2, B2, dW1, dB1, dW2, dB2, alpha)\n\n    if (i%20)==0:\n      print(\"Iteration number: \", i)\n      print(\"Accuracy = \", get_accuracy(get_predictions(A2), Y))\n  return W1, B1, W2, B2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:37:13.382259Z","iopub.execute_input":"2025-05-18T03:37:13.383041Z","iopub.status.idle":"2025-05-18T03:37:13.388682Z","shell.execute_reply.started":"2025-05-18T03:37:13.383015Z","shell.execute_reply":"2025-05-18T03:37:13.387819Z"}},"outputs":[],"execution_count":191},{"cell_type":"code","source":"W1, B1, W2, B2 = gradient_descent(X_train, Y_train, 0.1, 1001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:51:25.240393Z","iopub.execute_input":"2025-05-18T03:51:25.240719Z","iopub.status.idle":"2025-05-18T03:52:38.896488Z","shell.execute_reply.started":"2025-05-18T03:51:25.240695Z","shell.execute_reply":"2025-05-18T03:52:38.895250Z"}},"outputs":[{"name":"stdout","text":"Iteration number:  0\nAccuracy =  0.07228571428571429\nIteration number:  20\nAccuracy =  0.2329047619047619\nIteration number:  40\nAccuracy =  0.3975238095238095\nIteration number:  60\nAccuracy =  0.4915952380952381\nIteration number:  80\nAccuracy =  0.5512380952380952\nIteration number:  100\nAccuracy =  0.5998809523809524\nIteration number:  120\nAccuracy =  0.6417380952380952\nIteration number:  140\nAccuracy =  0.6797142857142857\nIteration number:  160\nAccuracy =  0.7081904761904761\nIteration number:  180\nAccuracy =  0.7297142857142858\nIteration number:  200\nAccuracy =  0.747\nIteration number:  220\nAccuracy =  0.7602142857142857\nIteration number:  240\nAccuracy =  0.7739285714285714\nIteration number:  260\nAccuracy =  0.7840238095238096\nIteration number:  280\nAccuracy =  0.7931904761904762\nIteration number:  300\nAccuracy =  0.8008333333333333\nIteration number:  320\nAccuracy =  0.8076190476190476\nIteration number:  340\nAccuracy =  0.8133095238095238\nIteration number:  360\nAccuracy =  0.8177857142857143\nIteration number:  380\nAccuracy =  0.8229761904761905\nIteration number:  400\nAccuracy =  0.8269047619047619\nIteration number:  420\nAccuracy =  0.8305714285714285\nIteration number:  440\nAccuracy =  0.833452380952381\nIteration number:  460\nAccuracy =  0.8363333333333334\nIteration number:  480\nAccuracy =  0.8390238095238095\nIteration number:  500\nAccuracy =  0.842\nIteration number:  520\nAccuracy =  0.8444047619047619\nIteration number:  540\nAccuracy =  0.8465952380952381\nIteration number:  560\nAccuracy =  0.8486666666666667\nIteration number:  580\nAccuracy =  0.8507619047619047\nIteration number:  600\nAccuracy =  0.8528809523809524\nIteration number:  620\nAccuracy =  0.8547142857142858\nIteration number:  640\nAccuracy =  0.8564761904761905\nIteration number:  660\nAccuracy =  0.8582619047619048\nIteration number:  680\nAccuracy =  0.8598333333333333\nIteration number:  700\nAccuracy =  0.8610476190476191\nIteration number:  720\nAccuracy =  0.8620476190476191\nIteration number:  740\nAccuracy =  0.8630714285714286\nIteration number:  760\nAccuracy =  0.8644761904761905\nIteration number:  780\nAccuracy =  0.8663095238095239\nIteration number:  800\nAccuracy =  0.8674047619047619\nIteration number:  820\nAccuracy =  0.8682619047619048\nIteration number:  840\nAccuracy =  0.8693809523809524\nIteration number:  860\nAccuracy =  0.870452380952381\nIteration number:  880\nAccuracy =  0.871452380952381\nIteration number:  900\nAccuracy =  0.8723095238095238\nIteration number:  920\nAccuracy =  0.8730952380952381\nIteration number:  940\nAccuracy =  0.8739523809523809\nIteration number:  960\nAccuracy =  0.8748571428571429\nIteration number:  980\nAccuracy =  0.8755\nIteration number:  1000\nAccuracy =  0.8763571428571428\n","output_type":"stream"}],"execution_count":217},{"cell_type":"markdown","source":"# Result\nOur overall accuracy is 87.6%, which means the model generalised quite well.","metadata":{}}]}